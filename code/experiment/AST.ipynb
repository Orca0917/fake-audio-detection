{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, feature_extractor, target_sampling_rate=16000):\n",
    "        self.file_paths = file_paths\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        data, sr = torchaudio.load(file_path)\n",
    "        data = torchaudio.functional.resample(data, orig_freq=sr, new_freq=self.target_sampling_rate)\n",
    "        data = data.squeeze()\n",
    "        inputs = self.feature_extractor(data, sampling_rate=self.target_sampling_rate, return_tensors=\"pt\")\n",
    "        inputs['input_values'] = inputs['input_values'].squeeze(0)  # Remove batch dimension\n",
    "        return inputs['input_values'], file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def inference_batch(model, dataloader, k=5, with_logit=False):\n",
    "    d = defaultdict(int)\n",
    "    counts = 0\n",
    "    tqdm_bar = tqdm(dataloader)\n",
    "\n",
    "    with open(\"/root/asset/test_only_speech_list_k5.txt\", \"w\") as tf:\n",
    "        for batch, paths in tqdm_bar:\n",
    "            batch = batch.to('cuda:0')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_values=batch).logits\n",
    "\n",
    "            for i, logits in enumerate(outputs):\n",
    "                logits = logits.squeeze()\n",
    "                predicted_class_ids = torch.argsort(logits)[-k:]\n",
    "                predicted_labels = [model.config.id2label[_id.item()] for _id in predicted_class_ids]\n",
    "\n",
    "                for label in predicted_labels:\n",
    "                    d[label] += 1\n",
    "\n",
    "                #list_of_lists = [str(tensor.tolist()) for tensor in sorted(logits)[-k:]]\n",
    "                sorted_indices = torch.argsort(logits)\n",
    "\n",
    "                # 0의 정렬된 인덱스에서의 위치를 찾음\n",
    "                sorted_position = (sorted_indices == 0).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "                # 뒤에서 몇 번째인지 계산\n",
    "                reverse_position = len(logits) - sorted_position - 1\n",
    "                zero_logit_value = logits[0].item()\n",
    "\n",
    "                tf.write(paths[i] + \" --> \" + str(reverse_position) + \" --> \" + str(zero_logit_value) + \"\\n\")\n",
    "                counts += 1\n",
    "                    \n",
    "\n",
    "            tqdm_bar.set_postfix(only_speech=d)\n",
    "\n",
    "    return d, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 82/1563 [00:58<17:44,  1.39it/s, only_speech=defaultdict(<class 'int'>, {'Breaking': 60, 'Bang': 5, 'Burst, pop': 31, 'Explosion': 48, 'Speech': 2348, 'Narration, monologue': 493, 'Female speech, woman speaking': 411, 'Inside, small room': 374, 'Animal': 419, 'Sliding door': 18, 'Door': 63, 'Stomach rumble': 12, 'Water': 110, 'Knock': 80, 'Slam': 16, 'Coin (dropping)': 24, 'Typing': 65, 'Computer keyboard': 76, 'Chop': 61, 'Bouncing': 18, 'Typewriter': 28, 'Scissors': 21, 'Pig': 36, 'Oink': 55, 'Grunt': 21, 'Music': 550, 'Helicopter': 31, 'Vehicle': 481, 'Speech synthesizer': 173, 'Conversation': 329, 'Tap': 44, 'Male speech, man speaking': 150, 'Applause': 54, 'Clapping': 61, 'Tick': 76, 'Tick-tock': 71, 'Fill (with liquid)': 41, 'Liquid': 90, 'Toilet flush': 43, 'Tools': 54, 'Power tool': 36, 'Wood': 77, 'Chainsaw': 38, 'Sigh': 29, 'Breathing': 27, 'Gasp': 74, 'Snort': 63, 'Chink, clink': 96, 'Crack': 43, 'Cap gun': 16, 'Sound effect': 126, 'Clock': 34, 'Cattle, bovinae': 50, 'Moo': 49, 'Livestock, farm animals, working animals': 90, 'Wind chime': 6, 'Chime': 4, 'Mosquito': 23, 'Bee, wasp, etc.': 30, 'Fly, housefly': 43, 'Insect': 68, 'Television': 38, 'Ping': 29, 'Rub': 78, 'Sawing': 61, 'Zipper (clothing)': 85, 'Coo': 8, 'Pigeon, dove': 6, 'Bird': 95, 'Squish': 10, 'Single-lens reflex camera': 90, 'Church bell': 45, 'Bell': 55, 'Change ringing (campanology)': 42, 'Jet engine': 34, 'Hair dryer': 25, 'Vacuum cleaner': 41, 'Caw': 61, 'Crow': 62, 'Mouse': 11, 'Cricket': 33, 'Toot': 27, 'Air horn, truck horn': 12, 'Vehicle horn, car horn, honking': 48, 'Outside, rural or natural': 15, 'Frog': 70, 'Raindrop': 39, 'Rain': 75, 'Rain on surface': 66, 'Goat': 10, 'Bleat': 61, 'Sheep': 58, 'Aircraft': 62, 'Boat, Water vehicle': 19, 'Motorboat, speedboat': 21, 'Car': 113, 'Eruption': 36, 'Domestic animals, pets': 104, 'Cat': 37, 'Meow': 36, 'Firecracker': 27, 'Fireworks': 30, 'Thunderstorm': 51, 'Thunder': 51, 'Siren': 68, 'Civil defense siren': 37, 'Emergency vehicle': 35, 'Chopping (food)': 57, 'Drip': 71, 'Buzzer': 26, 'Alarm clock': 29, 'Alarm': 33, 'Beep, bleep': 17, 'Fixed-wing aircraft, airplane': 24, 'Propeller, airscrew': 14, 'Silence': 98, 'Shatter': 24, 'Glass': 44, 'Sneeze': 6, 'Pant': 6, 'Roar': 11, 'Train': 61, 'Hammer': 4, 'Outside, urban or manmade': 52, 'Crunch': 23, 'Tearing': 6, 'Crumpling, crinkling': 13, 'Fire engine, fire truck (siren)': 24, 'Police car (siren)': 25, 'Yip': 3, 'Bow-wow': 38, 'Spray': 16, 'Shuffling cards': 5, 'Accelerating, revving, vroom': 22, 'Fowl': 91, 'Cluck': 79, 'Chicken, rooster': 81, 'Pink noise': 9, 'Wind': 29, 'Waves, surf': 43, 'Ocean': 45, 'Squeak': 26, 'Finger snapping': 17, 'Distortion': 1, 'Musical instrument': 11, 'Guitar': 1, 'Engine': 96, 'Heavy engine (low frequency)': 27, 'Microwave oven': 5, 'Wheeze': 6, 'Burping, eructation': 12, 'Railroad car, train wagon': 43, 'Subway, metro, underground': 2, 'Gurgling': 14, 'Crying, sobbing': 1, 'Baby cry, infant cry': 4, 'Crowing, cock-a-doodle-doo': 65, 'Throat clearing': 60, 'Aircraft engine': 8, 'Rattle': 25, 'Motorcycle': 13, 'Slosh': 14, 'Crackle': 23, 'Walk, footsteps': 19, 'Throbbing': 7, 'Heart murmur': 11, 'Heart sounds, heartbeat': 19, 'Sanding': 2, 'Toothbrush': 24, 'Medium engine (mid frequency)': 47, 'Idling': 48, 'Hoot': 8, 'Wind noise (microphone)': 31, 'Biting': 18, 'Blender': 13, 'Whir': 11, 'Field recording': 9, 'Filing (rasp)': 25, 'Rail transport': 44, 'Creak': 11, 'Clickety-clack': 10, 'Theremin': 3, 'Gunshot, gunfire': 13, 'Chirp, tweet': 27, 'Bird vocalization, bird call, bird song': 33, 'Croak': 41, 'Dog': 50, 'Cutlery, silverware': 14, 'Dishes, pots, and pans': 25, 'Smoke detector, smoke alarm': 7, 'Fire alarm': 4, 'Air conditioning': 17, 'Cowbell': 12, 'Plop': 25, 'Hands': 3, 'Flap': 7, 'Steam whistle': 2, 'Quack': 7, 'Pump (liquid)': 7, 'Vibration': 16, 'Whack, thwack': 3, 'Jingle bell': 3, 'Bicycle bell': 2, 'Car alarm': 9, 'Lawn mower': 1, 'Foghorn': 6, 'Owl': 29, 'Shuffle': 6, 'White noise': 17, 'Scrape': 7, 'Giggle': 2, 'Ding': 8, 'Fart': 10, 'Boing': 1, 'Honk': 1, 'Duck': 3, 'Buzz': 11, 'Water tap, faucet': 20, 'Wood block': 6, 'Screaming': 3, 'Squeal': 4, 'Roaring cats (lions, tigers)': 3, 'Run': 3, 'Bark': 6, 'Crowd': 3, 'Horse': 7, 'Clip-clop': 13, 'Camera': 7, 'Thump, thud': 1, 'Thunk': 6, 'Bus': 18, 'Gears': 9, 'Mechanisms': 21, 'Rumble': 6, 'Whale vocalization': 2, 'Clicking': 18, 'Truck': 10, 'Squawk': 8, 'Pour': 2, 'Ambulance (siren)': 17, 'Drum kit': 1, 'Drum': 12, 'Waterfall': 11, 'Stream': 4, 'Chewing, mastication': 5, 'Tubular bells': 1, 'Artillery fire': 7, 'Fire': 3, 'Patter': 3, 'Hum': 7, 'Splinter': 2, 'Whispering': 1, 'Sewing machine': 1, 'Slap, smack': 20, 'Gobble': 2, 'Turkey': 4, 'Telephone': 3, 'Telephone bell ringing': 6, 'Ringtone': 3, 'Smash, crash': 6, 'Caterwaul': 7, 'Reversing beeps': 1, 'Bathtub (filling or washing)': 1, 'Car passing by': 13, 'Bicycle': 2, 'Static': 4, 'Environmental noise': 4, 'Percussion': 3, 'Whip': 9, 'Wail, moan': 1, 'Clang': 5, 'Whimper': 3, 'Harmonica': 1, 'Cacophony': 4, 'Scratch': 5, 'Rattle (instrument)': 2, 'Printer': 1, 'Air brake': 2, 'Keys jangling': 1, 'Canidae, dogs, wolves': 1, 'Engine starting': 1, 'Snicker': 2, 'Neigh, whinny': 2, 'Basketball bounce': 2, 'Trickle, dribble': 5, 'Clatter': 4, 'Boiling': 11, 'Cash register': 4, 'Electric shaver, electric razor': 2, 'Electric toothbrush': 3, 'Rustling leaves': 5, 'Whoosh, swoosh, swish': 2, 'Male singing': 1, 'Gong': 2, 'Noise': 1, 'Crushing': 1, 'Motor vehicle (road)': 2, 'Inside, public space': 1, 'Ratchet, pawl': 2, 'Sine wave': 1, 'Whistle': 2, 'Maraca': 2, 'Steam': 1, 'Hiss': 1, 'Frying (food)': 1, 'Child speech, kid speaking': 2, 'Timpani': 1, 'Marimba, xylophone': 1, 'Bowed string instrument': 1, 'Double bass': 1, 'Cello': 1, 'Rustle': 2, 'Drill': 2, 'Mantra': 1, 'Chant': 1, 'Didgeridoo': 1, 'Chuckle, chortle': 1, 'Traffic noise, roadway noise': 1, 'Wild animals': 1, 'Drum roll': 1, 'Whistling': 1, 'Violin, fiddle': 1, 'Snake': 1, 'Brass instrument': 1})]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m d, counts \u001b[38;5;241m=\u001b[39m \u001b[43minference_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_logit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing. Total non-speech files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, counts)\n",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m, in \u001b[0;36minference_batch\u001b[0;34m(model, dataloader, k, with_logit)\u001b[0m\n\u001b[1;32m      5\u001b[0m tqdm_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/asset/test_only_speech_list_k5.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tf:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, paths \u001b[38;5;129;01min\u001b[39;00m tqdm_bar:\n\u001b[1;32m      9\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     12\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_paths[idx]\n\u001b[0;32m---> 13\u001b[0m     data, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     data \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mresample(data, orig_freq\u001b[38;5;241m=\u001b[39msr, new_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_sampling_rate)\n\u001b[1;32m     15\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:297\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    289\u001b[0m     uri: InputType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:91\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s\u001b[38;5;241m.\u001b[39mget_src_stream_info(s\u001b[38;5;241m.\u001b[39mdefault_audio_stream)\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n\u001b[0;32m---> 91\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43m_load_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m waveform, sample_rate\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:69\u001b[0m, in \u001b[0;36m_load_audio\u001b[0;34m(s, filter, channels_first)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_audio\u001b[39m(\n\u001b[1;32m     64\u001b[0m     s: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio.io.StreamReader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mfilter\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m     channels_first: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     68\u001b[0m     s\u001b[38;5;241m.\u001b[39madd_audio_stream(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, filter_desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_packets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mpop_chunks()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torio/io/_streaming_media_decoder.py:901\u001b[0m, in \u001b[0;36mStreamingMediaDecoder.process_all_packets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_all_packets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process packets until it reaches EOF.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_be\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_packets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model and feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = model.to('cuda:0')\n",
    "model.eval()\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32  # Adjust batch size according to your GPU memory\n",
    "file_paths = glob(\"/root/data/test/*.ogg\")\n",
    "dataset = AudioDataset(file_paths, feature_extractor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Run inference\n",
    "d, counts = inference_batch(model, dataloader, k=5, with_logit=True)\n",
    "print(\"Finished processing. Total non-speech files:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
